# AUTOGENERATED! DO NOT EDIT! File to edit: nbs/05_pipeline.ipynb (unless otherwise specified).

__all__ = ['download_eumetsat_files', 'df_metadata_to_dt_to_fp_map', 'reproject_datasets', 'compress_and_save_datasets',
           'download_between_data_range_pipeline', 'data_dir', 'metadata_db_fp', 'debug_fp', 'new_coords_fp',
           'new_grid_fp', 'zarr_bucket']

# Cell
import pandas as pd
import xarray as xr

from satip import eumetsat, reproj, io
from dagster import execute_pipeline, pipeline, solid, Field

import os
import dotenv
dotenv.load_dotenv('../.env')

data_dir = 'data/raw'
metadata_db_fp = 'data/EUMETSAT_metadata.db'
debug_fp = 'logs/EUMETSAT_download.txt'
new_coords_fp = '../data/intermediate/reproj_coords_TM_4km.csv'
new_grid_fp = '../data/intermediate/new_grid_4km_TM.json'
zarr_bucket = 'solar-pv-nowcasting-data/satellite/EUMETSAT/SEVIRI_RSS/full_extent_TM_zarr_int16'

@solid(
    config_schema = {
        'start_date': Field(str, is_required=True),
        'end_date': Field(str, is_required=True),
        'data_dir': Field(str, default_value=data_dir, is_required=False),
        'metadata_db_fp': Field(str, default_value=metadata_db_fp, is_required=False),
        'debug_fp': Field(str, default_value=debug_fp, is_required=False),
        'user_key': Field(str, default_value=os.environ.get('USER_KEY'), is_required=False),
        'user_secret': Field(str, default_value=os.environ.get('USER_SECRET'), is_required=False),
        'slack_webhook_url': Field(str, default_value=os.environ.get('SLACK_WEBHOOK_URL'), is_required=False),
        'slack_id': Field(str, default_value=os.environ.get('SLACK_ID'), is_required=False)
    }
)
def download_eumetsat_files(context, start_date: str, end_date: str):
    dm = eumetsat.DownloadManager(context.solid_config['user_key'], context.solid_config['user_secret'], context.solid_config['data_dir'], context.solid_config['metadata_db_fp'], context.solid_config['debug_fp'], slack_webhook_url=context.solid_config['slack_webhook_url'], slack_id=context.solid_config['slack_id'])

    df_new_metadata = dm.download_datasets(start_date, end_date)

    if df_new_metadata is None:
        df_new_metadata = pd.DataFrame(columns=['result_time', 'file_name'])

    return df_new_metadata

@solid(
    config_schema = {
        'data_dir': Field(str, default_value=data_dir, is_required=False)
    }
)
def df_metadata_to_dt_to_fp_map(context, df_new_metadata) -> dict:
    """
    Here we'll then identify downloaded files in
    the metadata dataframe and return a mapping
    between datetimes and filenames
    """

    datetime_to_filename = (df_new_metadata
                            .set_index('result_time')
                            ['file_name']
                            .drop_duplicates()
                            .to_dict()
                           )

    datetime_to_filepath = {
        datetime: f"{context.solid_config['data_dir']}/{filename}.nat"
        for datetime, filename
        in datetime_to_filename.items()
        if filename != {}
    }

    return datetime_to_filepath

@solid(
    config_schema = {
        'new_coords_fp': Field(str, default_value=new_coords_fp, is_required=False),
        'new_grid_fp': Field(str, default_value=new_grid_fp, is_required=False)
    }
)
def reproject_datasets(context, datetime_to_filepath: dict):
    reprojector = reproj.Reprojector(context.solid_config['new_coords_fp'], context.solid_config['new_grid_fp'])

    reprojected_dss = [
        (reprojector
         .reproject(filepath, reproj_library='pyinterp')
         .pipe(io.add_constant_coord_to_da, 'time', pd.to_datetime(datetime))
        )
        for datetime, filepath
        in datetime_to_filepath.items()
    ]

    ds_combined_reproj = xr.concat(reprojected_dss, 'time', coords='all', data_vars='all')

    return ds_combined_reproj

@solid(
    config_schema = {
        'zarr_bucket': Field(str, default_value=zarr_bucket, is_required=False),
        'var_name': Field(str, default_value='stacked_eumetsat_data', is_required=False)
    }
)
def compress_and_save_datasets(context, ds_combined_reproj):
    # Compressing the datasets
    compressor = io.Compressor()

    var_name = context.solid_config['var_name']
    da_compressed = compressor.compress(ds_combined_reproj[var_name])

    # Saving to Zarr
    io.save_da_to_zarr(da_compressed, context.solid_config['zarr_bucket'])

    return

@pipeline
def download_between_data_range_pipeline():
    df_new_metadata = download_eumetsat_files()
    datetime_to_filepath = df_metadata_to_dt_to_fp_map(df_new_metadata)
    ds_combined_reproj = reproject_datasets(datetime_to_filepath)
    compress_and_save_datasets(ds_combined_reproj)