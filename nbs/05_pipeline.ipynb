{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp mario"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End-to-End Pipeline\n",
    "\n",
    "<br>\n",
    "\n",
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exports\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "\n",
    "from satip import eumetsat, reproj, io, gcp_helpers\n",
    "from dagster import execute_pipeline, pipeline, solid, Field\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import dotenv\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Log Cleaning\n",
    "\n",
    "We'll suppress some errors/warnings to make the logs easier to parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exports\n",
    "warnings.filterwarnings('ignore', message='divide by zero encountered in true_divide')\n",
    "warnings.filterwarnings('ignore', message='invalid value encountered in sin')\n",
    "warnings.filterwarnings('ignore', message='invalid value encountered in cos')\n",
    "warnings.filterwarnings('ignore', message='You will likely lose important projection information when converting to a PROJ string from another format. See: https://proj.org/faq.html#what-is-the-best-format-for-describing-coordinate-reference-systems')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Dagster Pipeline\n",
    "\n",
    "We're now going to combine these steps into a pipeline using `dagster`, first we'll create the individual components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exports\n",
    "@solid()\n",
    "def download_eumetsat_files(context, env_vars_fp: str, data_dir: str, metadata_db_fp: str, debug_fp: str, table_id: str, project_id: str, start_date: str='', end_date: str=''):\n",
    "    _ = dotenv.load_dotenv(env_vars_fp)\n",
    "    \n",
    "    if start_date == '':\n",
    "        sql_query = f'select * from {table_id} where result_time = (select max(result_time) from {table_id})'\n",
    "        start_date = gcp_helpers.query(sql_query, project_id)['result_time'].iloc[0]\n",
    "        \n",
    "    if end_date == '':\n",
    "        end_date = pd.Timestamp.now().strftime('%Y-%m-%d %H:%M')\n",
    "        \n",
    "    context.log.info(f'Querying data between {start_date} - {end_date}')\n",
    "\n",
    "    dm = eumetsat.DownloadManager(os.environ.get('USER_KEY'), os.environ.get('USER_SECRET'), data_dir, metadata_db_fp, debug_fp, slack_webhook_url=os.environ.get('SLACK_WEBHOOK_URL'), slack_id=os.environ.get('SLACK_ID'))\n",
    "    df_new_metadata = dm.download_date_range(start_date, end_date)\n",
    "\n",
    "    if df_new_metadata is None:\n",
    "        df_new_metadata = pd.DataFrame(columns=['result_time', 'file_name'])\n",
    "    else:\n",
    "        df_new_metadata = df_new_metadata.iloc[1:] # the first entry is the last one we downloaded\n",
    "        \n",
    "    return df_new_metadata\n",
    "\n",
    "@solid()\n",
    "def df_metadata_to_dt_to_fp_map(_, df_new_metadata, data_dir: str) -> dict:\n",
    "    \"\"\"\n",
    "    Here we'll then identify downloaded files in \n",
    "    the metadata dataframe and return a mapping\n",
    "    between datetimes and filenames\n",
    "    \"\"\"\n",
    "    \n",
    "    datetime_to_filename = (df_new_metadata\n",
    "                            .set_index('result_time')\n",
    "                            ['file_name']\n",
    "                            .drop_duplicates()\n",
    "                            .to_dict()\n",
    "                           )\n",
    "\n",
    "    datetime_to_filepath = {\n",
    "        datetime: f\"{data_dir}/{filename}.nat\" \n",
    "        for datetime, filename \n",
    "        in datetime_to_filename.items()\n",
    "        if filename != {}\n",
    "    }\n",
    "    \n",
    "    return datetime_to_filepath\n",
    "\n",
    "@solid()\n",
    "def reproject_datasets(_, datetime_to_filepath: dict, new_coords_fp: str, new_grid_fp: str):\n",
    "    reprojector = reproj.Reprojector(new_coords_fp, new_grid_fp)\n",
    "\n",
    "    reprojected_dss = [\n",
    "        (reprojector\n",
    "         .reproject(filepath, reproj_library='pyresample')\n",
    "         .pipe(io.add_constant_coord_to_da, 'time', pd.to_datetime(datetime))\n",
    "        )\n",
    "        for datetime, filepath \n",
    "        in datetime_to_filepath.items()\n",
    "    ]\n",
    "\n",
    "    if len(reprojected_dss) > 0:\n",
    "        ds_combined_reproj = xr.concat(reprojected_dss, 'time', coords='all', data_vars='all')\n",
    "        return ds_combined_reproj\n",
    "    else:\n",
    "        return xr.Dataset()\n",
    "\n",
    "@solid()\n",
    "def compress_and_save_datasets(_, ds_combined_reproj, zarr_bucket: str, var_name: str='stacked_eumetsat_data'):\n",
    "    # Handle case where no new data exists\n",
    "    if len(ds_combined_reproj.dims) == 0:\n",
    "        return\n",
    "    \n",
    "    # Compressing the datasets\n",
    "    compressor = io.Compressor()\n",
    "\n",
    "    var_name = var_name\n",
    "    da_compressed = compressor.compress(ds_combined_reproj[var_name])\n",
    "\n",
    "    # Saving to Zarr\n",
    "    ds_compressed = io.save_da_to_zarr(da_compressed, zarr_bucket)\n",
    "    \n",
    "    return ds_compressed\n",
    "\n",
    "@solid()\n",
    "def save_metadata(context, ds_combined_compressed, df_new_metadata, table_id: str, project_id: str):\n",
    "    if ds_combined_compressed is not None:\n",
    "        if df_new_metadata.shape[0] > 0:\n",
    "            gcp_helpers.write_metadata_to_gcp(df_new_metadata, table_id, project_id, append=True)\n",
    "            context.log.info(f'{df_new_metadata.shape[0]} new metadata entries were added')\n",
    "        else:\n",
    "            context.log.info('No metadata was available to be added')\n",
    "            \n",
    "    return True\n",
    "\n",
    "@solid()\n",
    "def compress_export_then_delete_raw(context, ds_combined_compressed, data_dir: str, compressed_dir: str, BUCKET_NAME: str='solar-pv-nowcasting-data', PREFIX: str='satellite/EUMETSAT/SEVIRI_RSS/native/', ready_to_delete: bool=True):\n",
    "    if ready_to_delete == True:\n",
    "        eumetsat.compress_downloaded_files(data_dir=data_dir, compressed_dir=compressed_dir, log=context.log)\n",
    "        eumetsat.upload_compressed_files(compressed_dir, BUCKET_NAME=BUCKET_NAME, PREFIX=PREFIX, log=None)\n",
    "        \n",
    "        for dir_ in [data_dir, compressed_dir]:\n",
    "            files = glob.glob(f'{dir_}/*')\n",
    "            \n",
    "            for f in files:\n",
    "                os.remove(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "Then we'll combine them in a pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exports\n",
    "@pipeline\n",
    "def download_latest_data_pipeline(): \n",
    "    df_new_metadata = download_eumetsat_files()\n",
    "    datetime_to_filepath = df_metadata_to_dt_to_fp_map(df_new_metadata)\n",
    "    ds_combined_reproj = reproject_datasets(datetime_to_filepath)\n",
    "    ds_combined_compressed = compress_and_save_datasets(ds_combined_reproj)\n",
    "    \n",
    "    ready_to_delete = save_metadata(ds_combined_compressed, df_new_metadata)\n",
    "    compress_export_then_delete_raw(ready_to_delete)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "Which we'll now run a test with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_config = {\n",
    "    'solids': {\n",
    "        'download_eumetsat_files': {\n",
    "            'inputs': {\n",
    "                'env_vars_fp': \"../.env\",\n",
    "                'data_dir': \"../data/raw\",\n",
    "                'metadata_db_fp': \"../data/EUMETSAT_metadata.db\",\n",
    "                'debug_fp': \"../logs/EUMETSAT_download.txt\",\n",
    "                'table_id': \"eumetsat.metadata\",\n",
    "                'project_id': \"solar-pv-nowcasting\",\n",
    "                'start_date': \"\",\n",
    "                'end_date': \"\"\n",
    "            },\n",
    "        },\n",
    "        'df_metadata_to_dt_to_fp_map': {\n",
    "            'inputs': {\n",
    "                'data_dir': \"../data/raw\"\n",
    "            }\n",
    "        },\n",
    "        'reproject_datasets': {\n",
    "            'inputs': {\n",
    "                'new_coords_fp': \"../data/intermediate/reproj_coords_TM_4km.csv\",\n",
    "                'new_grid_fp': \"../data/intermediate/new_grid_4km_TM.json\"\n",
    "            }\n",
    "        },\n",
    "        'compress_and_save_datasets': {\n",
    "            'inputs': {\n",
    "                'zarr_bucket': \"solar-pv-nowcasting-data/satellite/EUMETSAT/SEVIRI_RSS/full_extent_TM_int16\",\n",
    "                'var_name': \"stacked_eumetsat_data\"\n",
    "            }\n",
    "        },\n",
    "        'save_metadata': {\n",
    "            'inputs': {\n",
    "                'table_id': \"eumetsat.metadata\",\n",
    "                'project_id': \"solar-pv-nowcasting\"\n",
    "            },\n",
    "        },\n",
    "        'compress_export_then_delete_raw': {\n",
    "            'inputs': {\n",
    "                'data_dir': \"../data/raw\",\n",
    "                'compressed_dir': \"../data/compressed\",\n",
    "                'BUCKET_NAME': \"solar-pv-nowcasting-data\",\n",
    "                'PREFIX': \"satellite/EUMETSAT/SEVIRI_RSS/native/\",\n",
    "                'ready_to_delete': True\n",
    "            },\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# execute_pipeline(download_latest_data_pipeline, run_config=run_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exports\n",
    "@solid()\n",
    "def download_missing_eumetsat_files(context, env_vars_fp: str, data_dir: str, metadata_db_fp: str, debug_fp: str, table_id: str, project_id: str, start_date: str='', end_date: str=''):\n",
    "    _ = dotenv.load_dotenv(env_vars_fp)\n",
    "    dm = eumetsat.DownloadManager(os.environ.get('USER_KEY'), os.environ.get('USER_SECRET'), data_dir, metadata_db_fp, debug_fp, slack_webhook_url=os.environ.get('SLACK_WEBHOOK_URL'), slack_id=os.environ.get('SLACK_ID'))\n",
    "    \n",
    "    missing_datasets = io.identifying_missing_datasets(start_date, end_date)[:5] # <- remove after tests\n",
    "    df_new_metadata = dm.download_datasets(missing_datasets)\n",
    "\n",
    "    if df_new_metadata is None:\n",
    "        df_new_metadata = pd.DataFrame(columns=['result_time', 'file_name'])\n",
    "    else:\n",
    "        df_new_metadata = df_new_metadata.iloc[1:] # the first entry is the last one we downloaded\n",
    "    \n",
    "    return df_new_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "@pipeline\n",
    "def download_missing_data_pipeline():  \n",
    "    # Retrieving data, reprojecting, compressing, and saving to GCP\n",
    "    df_new_metadata = download_missing_eumetsat_files()\n",
    "    datetime_to_filepath = df_metadata_to_dt_to_fp_map(df_new_metadata)\n",
    "    ds_combined_reproj = reproject_datasets(datetime_to_filepath)\n",
    "    ds_combined_compressed = compress_and_save_datasets(ds_combined_reproj)\n",
    "    \n",
    "    save_metadata(ds_combined_compressed, df_new_metadata)\n",
    "    compress_export_then_delete_raw(ds_combined_compressed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "if 'download_eumetsat_files' in run_config['solids'].keys():\n",
    "    run_config['solids']['download_missing_eumetsat_files'] = run_config['solids']['download_eumetsat_files']\n",
    "    run_config['solids'].pop('download_eumetsat_files')\n",
    "\n",
    "# execute_pipeline(download_missing_data_pipeline, run_config=run_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_utils.ipynb.\n",
      "Converted 01_eumetsat.ipynb.\n",
      "Converted 02_reprojection.ipynb.\n",
      "Converted 03_zarr.ipynb.\n",
      "Converted 04_gcp.ipynb.\n",
      "Converted 05_pipeline.ipynb.\n",
      "Converted 101_downloading.ipynb.\n",
      "Converted 102_reprojecting.ipynb.\n",
      "Converted 103_loading.ipynb.\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "from nbdev.export import *\n",
    "notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "satip_dev",
   "language": "python",
   "name": "satip_dev"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
