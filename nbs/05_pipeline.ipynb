{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp mario"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End-to-End Pipeline\n",
    "\n",
    "<br>\n",
    "\n",
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ayrto\\anaconda3\\envs\\satip_dev\\lib\\site-packages\\google\\auth\\_default.py:69: UserWarning: Your application has authenticated using end user credentials from Google Cloud SDK without a quota project. You might receive a \"quota exceeded\" or \"API not enabled\" error. We recommend you rerun `gcloud auth application-default login` and make sure a quota project is added. Or you can use service accounts instead. For more information about service accounts, see https://cloud.google.com/docs/authentication/\n",
      "  warnings.warn(_CLOUD_SDK_CREDENTIALS_WARNING)\n",
      "Downloading: 100%|█████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  3.77rows/s]\n",
      "c:\\users\\ayrto\\desktop\\freelance work\\fea\\work\\ocf\\satip\\satip\\io.py:172: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  if (start_date is '') or (end_date is ''):\n",
      "c:\\users\\ayrto\\desktop\\freelance work\\fea\\work\\ocf\\satip\\satip\\io.py:172: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  if (start_date is '') or (end_date is ''):\n"
     ]
    }
   ],
   "source": [
    "#exports\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "\n",
    "from satip import eumetsat, reproj, io, gcp_helpers\n",
    "from dagster import execute_pipeline, pipeline, solid, Field\n",
    "\n",
    "import os\n",
    "import dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### User Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '../data/raw'\n",
    "sorted_dir = '../data/sorted'\n",
    "debug_fp = '../logs/EUMETSAT_download.txt'\n",
    "env_vars_fp = '../.env'\n",
    "metadata_db_fp = '../data/EUMETSAT_metadata.db'\n",
    "new_grid_fp='../data/intermediate/new_grid_4km_TM.json'\n",
    "new_coords_fp = '../data/intermediate/reproj_coords_TM_4km.csv'\n",
    "zarr_bucket = 'solar-pv-nowcasting-data/satellite/EUMETSAT/SEVIRI_RSS/full_extent_TM_int16'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Loading Environment Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "_ = dotenv.load_dotenv(env_vars_fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exports\n",
    "user_key = os.environ.get('USER_KEY')\n",
    "user_secret = os.environ.get('USER_SECRET')\n",
    "slack_id = os.environ.get('SLACK_ID')\n",
    "slack_webhook_url = os.environ.get('SLACK_WEBHOOK_URL')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Dagster Pipeline\n",
    "\n",
    "We're now going to combine these steps into a pipeline using `dagster`, first we'll create the individual components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exports\n",
    "@solid(\n",
    "    config_schema = {\n",
    "        'user_key': Field(str, default_value=user_key, is_required=False),\n",
    "        'user_secret': Field(str, default_value=user_secret, is_required=False),\n",
    "        'slack_webhook_url': Field(str, default_value=slack_webhook_url, is_required=False),\n",
    "        'slack_id': Field(str, default_value=slack_id, is_required=False)\n",
    "    }\n",
    ")\n",
    "def download_eumetsat_files(context, data_dir: str, metadata_db_fp: str, debug_fp: str, table_id: str, project_id: str, start_date: str='', end_date: str=''):\n",
    "    if start_date == '':\n",
    "        sql_query = f'select * from {table_id} where result_time = (select max(result_time) from {table_id})'\n",
    "        start_date = gcp_helpers.query(sql_query, project_id)['result_time'].iloc[0]\n",
    "        \n",
    "    if end_date == '':\n",
    "        end_date = pd.Timestamp.now().strftime('%Y-%m-%d %H:%M')\n",
    "\n",
    "    dm = eumetsat.DownloadManager(context.solid_config['user_key'], context.solid_config['user_secret'], data_dir, metadata_db_fp, debug_fp, slack_webhook_url=context.solid_config['slack_webhook_url'], slack_id=context.solid_config['slack_id'])\n",
    "    df_new_metadata = dm.download_date_range(start_date, end_date)\n",
    "\n",
    "    if df_new_metadata is None:\n",
    "        df_new_metadata = pd.DataFrame(columns=['result_time', 'file_name'])\n",
    "    else:\n",
    "        df_new_metadata = df_new_metadata.iloc[1:] # the first entry is the last one we downloaded\n",
    "        \n",
    "    return df_new_metadata\n",
    "\n",
    "@solid()\n",
    "def df_metadata_to_dt_to_fp_map(_, df_new_metadata, data_dir: str) -> dict:\n",
    "    \"\"\"\n",
    "    Here we'll then identify downloaded files in \n",
    "    the metadata dataframe and return a mapping\n",
    "    between datetimes and filenames\n",
    "    \"\"\"\n",
    "    \n",
    "    datetime_to_filename = (df_new_metadata\n",
    "                            .set_index('result_time')\n",
    "                            ['file_name']\n",
    "                            .drop_duplicates()\n",
    "                            .to_dict()\n",
    "                           )\n",
    "\n",
    "    datetime_to_filepath = {\n",
    "        datetime: f\"{data_dir}/{filename}.nat\" \n",
    "        for datetime, filename \n",
    "        in datetime_to_filename.items()\n",
    "        if filename != {}\n",
    "    }\n",
    "    \n",
    "    return datetime_to_filepath\n",
    "\n",
    "@solid()\n",
    "def reproject_datasets(_, datetime_to_filepath: dict, new_coords_fp: str, new_grid_fp: str):\n",
    "    reprojector = reproj.Reprojector(new_coords_fp, new_grid_fp)\n",
    "\n",
    "    reprojected_dss = [\n",
    "        (reprojector\n",
    "         .reproject(filepath, reproj_library='pyresample')\n",
    "         .pipe(io.add_constant_coord_to_da, 'time', pd.to_datetime(datetime))\n",
    "        )\n",
    "        for datetime, filepath \n",
    "        in datetime_to_filepath.items()\n",
    "    ]\n",
    "\n",
    "    if len(reprojected_dss) > 0:\n",
    "        ds_combined_reproj = xr.concat(reprojected_dss, 'time', coords='all', data_vars='all')\n",
    "        return ds_combined_reproj\n",
    "    else:\n",
    "        return xr.Dataset()\n",
    "\n",
    "@solid()\n",
    "def compress_and_save_datasets(_, ds_combined_reproj, zarr_bucket: str, var_name: str='stacked_eumetsat_data'):\n",
    "    # Handle case where no new data exists\n",
    "    if len(ds_combined_reproj.dims) == 0:\n",
    "        return\n",
    "    \n",
    "    # Compressing the datasets\n",
    "    compressor = io.Compressor()\n",
    "\n",
    "    var_name = var_name\n",
    "    da_compressed = compressor.compress(ds_combined_reproj[var_name])\n",
    "\n",
    "    # Saving to Zarr\n",
    "    ds_compressed = io.save_da_to_zarr(da_compressed, zarr_bucket)\n",
    "    \n",
    "    return ds_compressed\n",
    "\n",
    "@solid()\n",
    "def save_metadata(context, ds_combined_compressed, df_new_metadata, table_id: str, project_id: str):\n",
    "    if ds_combined_compressed is not None:\n",
    "        if df_new_metadata.shape[0] > 0:\n",
    "            gcp_helpers.write_metadata_to_gcp(df_new_metadata, table_id, project_id, append=True)\n",
    "            context.log.info(f'{df_new_metadata.shape[0]} new metadata entries were added')\n",
    "        else:\n",
    "            context.log.info('No metadata was available to be added')\n",
    "\n",
    "@solid()\n",
    "def compress_export_then_delete_raw(context, ds_combined_compressed, data_dir: str, compressed_dir: str, BUCKET_NAME: str='solar-pv-nowcasting-data', PREFIX: str='satellite/EUMETSAT/SEVIRI_RSS/native/'):\n",
    "    if ds_combined_compressed is not None:\n",
    "        eumetsat.compress_downloaded_files(data_dir=data_dir, compressed_dir=compressed_dir, log=context.log)\n",
    "        eumetsat.upload_compressed_files(compressed_dir, BUCKET_NAME=BUCKET_NAME, PREFIX=PREFIX, log=None)\n",
    "        \n",
    "        for dir_ in [data_dir, compressed_dir]:\n",
    "            files = glob.glob(f'{dir_}/*')\n",
    "            \n",
    "            for f in files:\n",
    "                os.remove(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "Then we'll combine them in a pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exports\n",
    "@pipeline\n",
    "def download_latest_data_pipeline():  \n",
    "    # Retrieving data, reprojecting, compressing, and saving to GCP\n",
    "    df_new_metadata = download_eumetsat_files()\n",
    "    datetime_to_filepath = df_metadata_to_dt_to_fp_map(df_new_metadata)\n",
    "    ds_combined_reproj = reproject_datasets(datetime_to_filepath)\n",
    "    ds_combined_compressed = compress_and_save_datasets(ds_combined_reproj)\n",
    "    \n",
    "    save_metadata(ds_combined_compressed, df_new_metadata)\n",
    "    compress_export_then_delete_raw(ds_combined_compressed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "Which we'll now run a test with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-07 19:01:59 - dagster - DEBUG - download_latest_data_pipeline - b5c09304-46d3-491d-b84f-7f3c41ceb9c2 - 8628 - ENGINE_EVENT - Starting initialization of resources [asset_store].\n",
      "2021-01-07 19:01:59 - dagster - DEBUG - download_latest_data_pipeline - b5c09304-46d3-491d-b84f-7f3c41ceb9c2 - 8628 - ENGINE_EVENT - Finished initialization of resources [asset_store].\n",
      "2021-01-07 19:01:59 - dagster - DEBUG - download_latest_data_pipeline - b5c09304-46d3-491d-b84f-7f3c41ceb9c2 - 8628 - PIPELINE_START - Started execution of pipeline \"download_latest_data_pipeline\".\n",
      "2021-01-07 19:01:59 - dagster - DEBUG - download_latest_data_pipeline - b5c09304-46d3-491d-b84f-7f3c41ceb9c2 - 8628 - ENGINE_EVENT - Executing steps in process (pid: 8628)\n",
      "2021-01-07 19:01:59 - dagster - DEBUG - download_latest_data_pipeline - b5c09304-46d3-491d-b84f-7f3c41ceb9c2 - 8628 - download_eumetsat_files.compute - STEP_START - Started execution of step \"download_eumetsat_files.compute\".\n",
      "2021-01-07 19:01:59 - dagster - DEBUG - download_latest_data_pipeline - b5c09304-46d3-491d-b84f-7f3c41ceb9c2 - 8628 - download_eumetsat_files.compute - STEP_INPUT - Got input \"data_dir\" of type \"String\". (Type check passed).\n",
      "2021-01-07 19:01:59 - dagster - DEBUG - download_latest_data_pipeline - b5c09304-46d3-491d-b84f-7f3c41ceb9c2 - 8628 - download_eumetsat_files.compute - STEP_INPUT - Got input \"metadata_db_fp\" of type \"String\". (Type check passed).\n",
      "2021-01-07 19:01:59 - dagster - DEBUG - download_latest_data_pipeline - b5c09304-46d3-491d-b84f-7f3c41ceb9c2 - 8628 - download_eumetsat_files.compute - STEP_INPUT - Got input \"debug_fp\" of type \"String\". (Type check passed).\n",
      "2021-01-07 19:01:59 - dagster - DEBUG - download_latest_data_pipeline - b5c09304-46d3-491d-b84f-7f3c41ceb9c2 - 8628 - download_eumetsat_files.compute - STEP_INPUT - Got input \"table_id\" of type \"String\". (Type check passed).\n",
      "2021-01-07 19:01:59 - dagster - DEBUG - download_latest_data_pipeline - b5c09304-46d3-491d-b84f-7f3c41ceb9c2 - 8628 - download_eumetsat_files.compute - STEP_INPUT - Got input \"project_id\" of type \"String\". (Type check passed).\n",
      "2021-01-07 19:01:59 - dagster - DEBUG - download_latest_data_pipeline - b5c09304-46d3-491d-b84f-7f3c41ceb9c2 - 8628 - download_eumetsat_files.compute - STEP_INPUT - Got input \"start_date\" of type \"String\". (Type check passed).\n",
      "2021-01-07 19:01:59 - dagster - DEBUG - download_latest_data_pipeline - b5c09304-46d3-491d-b84f-7f3c41ceb9c2 - 8628 - download_eumetsat_files.compute - STEP_INPUT - Got input \"end_date\" of type \"String\". (Type check passed).\n",
      "Downloading: 100%|█████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  3.19rows/s]\n",
      "2021-01-07 19:02:01,870 - INFO - ********** Download Manager Initialised **************\n",
      "2021-01-07 19:02:01,870 - INFO - ********** Download Manager Initialised **************\n",
      "2021-01-07 19:02:01,870 - INFO - ********** Download Manager Initialised **************\n",
      "2021-01-07 19:02:02,316 - INFO - 1 files queried, 0 found in ../data/raw, 1 to download.\n",
      "2021-01-07 19:02:02,316 - INFO - 1 files queried, 0 found in ../data/raw, 1 to download.\n",
      "2021-01-07 19:02:02,316 - INFO - 1 files queried, 0 found in ../data/raw, 1 to download.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><span class=\"Text-label\" style=\"display:inline-block; overflow:hidden; white-space:nowrap; text-overflow:ellipsis; min-width:0; max-width:15ex; vertical-align:middle; text-align:right\"></span>\n",
       "<progress style=\"width:60ex\" max=\"1\" value=\"1\" class=\"Progress-main\"/></progress>\n",
       "<span class=\"Progress-label\"><strong>100%</strong></span>\n",
       "<span class=\"Iteration-label\">1/1</span>\n",
       "<span class=\"Time-label\">[00:06<00:06, 5.83s/it]</span></div>"
      ],
      "text/plain": [
       "\u001b[A\u001b[2K\r",
       " [████████████████████████████████████████████████████████████] 1/1 [00:06<00:06, 5.83s/it]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-07 19:02:08 - dagster - DEBUG - download_latest_data_pipeline - b5c09304-46d3-491d-b84f-7f3c41ceb9c2 - 8628 - download_eumetsat_files.compute - STEP_OUTPUT - Yielded output \"result\" of type \"Any\". (Type check passed).\n",
      "2021-01-07 19:02:08 - dagster - DEBUG - download_latest_data_pipeline - b5c09304-46d3-491d-b84f-7f3c41ceb9c2 - 8628 - download_eumetsat_files.compute - OBJECT_STORE_OPERATION - Stored intermediate object for output result in memory object store using pickle.\n",
      "2021-01-07 19:02:08 - dagster - DEBUG - download_latest_data_pipeline - b5c09304-46d3-491d-b84f-7f3c41ceb9c2 - 8628 - download_eumetsat_files.compute - STEP_SUCCESS - Finished execution of step \"download_eumetsat_files.compute\" in 8.32s.\n",
      "2021-01-07 19:02:08 - dagster - DEBUG - download_latest_data_pipeline - b5c09304-46d3-491d-b84f-7f3c41ceb9c2 - 8628 - df_metadata_to_dt_to_fp_map.compute - STEP_START - Started execution of step \"df_metadata_to_dt_to_fp_map.compute\".\n",
      "2021-01-07 19:02:08 - dagster - DEBUG - download_latest_data_pipeline - b5c09304-46d3-491d-b84f-7f3c41ceb9c2 - 8628 - df_metadata_to_dt_to_fp_map.compute - OBJECT_STORE_OPERATION - Retrieved intermediate object for input df_new_metadata in memory object store using pickle.\n",
      "2021-01-07 19:02:08 - dagster - DEBUG - download_latest_data_pipeline - b5c09304-46d3-491d-b84f-7f3c41ceb9c2 - 8628 - df_metadata_to_dt_to_fp_map.compute - STEP_INPUT - Got input \"df_new_metadata\" of type \"Any\". (Type check passed).\n",
      "2021-01-07 19:02:08 - dagster - DEBUG - download_latest_data_pipeline - b5c09304-46d3-491d-b84f-7f3c41ceb9c2 - 8628 - df_metadata_to_dt_to_fp_map.compute - STEP_INPUT - Got input \"data_dir\" of type \"String\". (Type check passed).\n",
      "2021-01-07 19:02:08 - dagster - DEBUG - download_latest_data_pipeline - b5c09304-46d3-491d-b84f-7f3c41ceb9c2 - 8628 - df_metadata_to_dt_to_fp_map.compute - STEP_OUTPUT - Yielded output \"result\" of type \"dict\". (Type check passed).\n",
      "2021-01-07 19:02:08 - dagster - DEBUG - download_latest_data_pipeline - b5c09304-46d3-491d-b84f-7f3c41ceb9c2 - 8628 - df_metadata_to_dt_to_fp_map.compute - OBJECT_STORE_OPERATION - Stored intermediate object for output result in memory object store using pickle.\n",
      "2021-01-07 19:02:08 - dagster - DEBUG - download_latest_data_pipeline - b5c09304-46d3-491d-b84f-7f3c41ceb9c2 - 8628 - df_metadata_to_dt_to_fp_map.compute - STEP_SUCCESS - Finished execution of step \"df_metadata_to_dt_to_fp_map.compute\" in 8.33ms.\n",
      "2021-01-07 19:02:08 - dagster - DEBUG - download_latest_data_pipeline - b5c09304-46d3-491d-b84f-7f3c41ceb9c2 - 8628 - reproject_datasets.compute - STEP_START - Started execution of step \"reproject_datasets.compute\".\n",
      "2021-01-07 19:02:08 - dagster - DEBUG - download_latest_data_pipeline - b5c09304-46d3-491d-b84f-7f3c41ceb9c2 - 8628 - reproject_datasets.compute - OBJECT_STORE_OPERATION - Retrieved intermediate object for input datetime_to_filepath in memory object store using pickle.\n",
      "2021-01-07 19:02:08 - dagster - DEBUG - download_latest_data_pipeline - b5c09304-46d3-491d-b84f-7f3c41ceb9c2 - 8628 - reproject_datasets.compute - STEP_INPUT - Got input \"datetime_to_filepath\" of type \"dict\". (Type check passed).\n",
      "2021-01-07 19:02:08 - dagster - DEBUG - download_latest_data_pipeline - b5c09304-46d3-491d-b84f-7f3c41ceb9c2 - 8628 - reproject_datasets.compute - STEP_INPUT - Got input \"new_coords_fp\" of type \"String\". (Type check passed).\n",
      "2021-01-07 19:02:08 - dagster - DEBUG - download_latest_data_pipeline - b5c09304-46d3-491d-b84f-7f3c41ceb9c2 - 8628 - reproject_datasets.compute - STEP_INPUT - Got input \"new_grid_fp\" of type \"String\". (Type check passed).\n",
      "2021-01-07 19:02:10 - dagster - DEBUG - download_latest_data_pipeline - b5c09304-46d3-491d-b84f-7f3c41ceb9c2 - 8628 - reproject_datasets.compute - STEP_OUTPUT - Yielded output \"result\" of type \"Any\". (Type check passed).\n",
      "2021-01-07 19:02:10 - dagster - DEBUG - download_latest_data_pipeline - b5c09304-46d3-491d-b84f-7f3c41ceb9c2 - 8628 - reproject_datasets.compute - OBJECT_STORE_OPERATION - Stored intermediate object for output result in memory object store using pickle.\n",
      "2021-01-07 19:02:10 - dagster - DEBUG - download_latest_data_pipeline - b5c09304-46d3-491d-b84f-7f3c41ceb9c2 - 8628 - reproject_datasets.compute - STEP_SUCCESS - Finished execution of step \"reproject_datasets.compute\" in 1.88s.\n",
      "2021-01-07 19:02:10 - dagster - DEBUG - download_latest_data_pipeline - b5c09304-46d3-491d-b84f-7f3c41ceb9c2 - 8628 - compress_and_save_datasets.compute - STEP_START - Started execution of step \"compress_and_save_datasets.compute\".\n",
      "2021-01-07 19:02:10 - dagster - DEBUG - download_latest_data_pipeline - b5c09304-46d3-491d-b84f-7f3c41ceb9c2 - 8628 - compress_and_save_datasets.compute - OBJECT_STORE_OPERATION - Retrieved intermediate object for input ds_combined_reproj in memory object store using pickle.\n",
      "2021-01-07 19:02:10 - dagster - DEBUG - download_latest_data_pipeline - b5c09304-46d3-491d-b84f-7f3c41ceb9c2 - 8628 - compress_and_save_datasets.compute - STEP_INPUT - Got input \"ds_combined_reproj\" of type \"Any\". (Type check passed).\n",
      "2021-01-07 19:02:10 - dagster - DEBUG - download_latest_data_pipeline - b5c09304-46d3-491d-b84f-7f3c41ceb9c2 - 8628 - compress_and_save_datasets.compute - STEP_INPUT - Got input \"zarr_bucket\" of type \"String\". (Type check passed).\n",
      "2021-01-07 19:02:10 - dagster - DEBUG - download_latest_data_pipeline - b5c09304-46d3-491d-b84f-7f3c41ceb9c2 - 8628 - compress_and_save_datasets.compute - STEP_INPUT - Got input \"var_name\" of type \"String\". (Type check passed).\n",
      "2021-01-07 19:02:10 - dagster - DEBUG - download_latest_data_pipeline - b5c09304-46d3-491d-b84f-7f3c41ceb9c2 - 8628 - compress_and_save_datasets.compute - STEP_OUTPUT - Yielded output \"result\" of type \"Any\". (Type check passed).\n",
      "2021-01-07 19:02:10 - dagster - DEBUG - download_latest_data_pipeline - b5c09304-46d3-491d-b84f-7f3c41ceb9c2 - 8628 - compress_and_save_datasets.compute - OBJECT_STORE_OPERATION - Stored intermediate object for output result in memory object store using pickle.\n",
      "2021-01-07 19:02:10 - dagster - DEBUG - download_latest_data_pipeline - b5c09304-46d3-491d-b84f-7f3c41ceb9c2 - 8628 - compress_and_save_datasets.compute - STEP_SUCCESS - Finished execution of step \"compress_and_save_datasets.compute\" in 6.19ms.\n",
      "2021-01-07 19:02:10 - dagster - DEBUG - download_latest_data_pipeline - b5c09304-46d3-491d-b84f-7f3c41ceb9c2 - 8628 - compress_export_then_delete_raw.compute - STEP_START - Started execution of step \"compress_export_then_delete_raw.compute\".\n",
      "2021-01-07 19:02:10 - dagster - DEBUG - download_latest_data_pipeline - b5c09304-46d3-491d-b84f-7f3c41ceb9c2 - 8628 - compress_export_then_delete_raw.compute - OBJECT_STORE_OPERATION - Retrieved intermediate object for input ds_combined_compressed in memory object store using pickle.\n",
      "2021-01-07 19:02:10 - dagster - DEBUG - download_latest_data_pipeline - b5c09304-46d3-491d-b84f-7f3c41ceb9c2 - 8628 - compress_export_then_delete_raw.compute - STEP_INPUT - Got input \"ds_combined_compressed\" of type \"Any\". (Type check passed).\n",
      "2021-01-07 19:02:10 - dagster - DEBUG - download_latest_data_pipeline - b5c09304-46d3-491d-b84f-7f3c41ceb9c2 - 8628 - compress_export_then_delete_raw.compute - STEP_INPUT - Got input \"data_dir\" of type \"String\". (Type check passed).\n",
      "2021-01-07 19:02:10 - dagster - DEBUG - download_latest_data_pipeline - b5c09304-46d3-491d-b84f-7f3c41ceb9c2 - 8628 - compress_export_then_delete_raw.compute - STEP_INPUT - Got input \"compressed_dir\" of type \"String\". (Type check passed).\n",
      "2021-01-07 19:02:10 - dagster - DEBUG - download_latest_data_pipeline - b5c09304-46d3-491d-b84f-7f3c41ceb9c2 - 8628 - compress_export_then_delete_raw.compute - STEP_INPUT - Got input \"BUCKET_NAME\" of type \"String\". (Type check passed).\n",
      "2021-01-07 19:02:10 - dagster - DEBUG - download_latest_data_pipeline - b5c09304-46d3-491d-b84f-7f3c41ceb9c2 - 8628 - compress_export_then_delete_raw.compute - STEP_INPUT - Got input \"PREFIX\" of type \"String\". (Type check passed).\n",
      "2021-01-07 19:02:10 - dagster - DEBUG - download_latest_data_pipeline - b5c09304-46d3-491d-b84f-7f3c41ceb9c2 - 8628 - compress_export_then_delete_raw.compute - STEP_OUTPUT - Yielded output \"result\" of type \"Any\". (Type check passed).\n",
      "2021-01-07 19:02:10 - dagster - DEBUG - download_latest_data_pipeline - b5c09304-46d3-491d-b84f-7f3c41ceb9c2 - 8628 - compress_export_then_delete_raw.compute - OBJECT_STORE_OPERATION - Stored intermediate object for output result in memory object store using pickle.\n",
      "2021-01-07 19:02:10 - dagster - DEBUG - download_latest_data_pipeline - b5c09304-46d3-491d-b84f-7f3c41ceb9c2 - 8628 - compress_export_then_delete_raw.compute - STEP_SUCCESS - Finished execution of step \"compress_export_then_delete_raw.compute\" in 9.29ms.\n",
      "2021-01-07 19:02:10 - dagster - DEBUG - download_latest_data_pipeline - b5c09304-46d3-491d-b84f-7f3c41ceb9c2 - 8628 - save_metadata.compute - STEP_START - Started execution of step \"save_metadata.compute\".\n",
      "2021-01-07 19:02:10 - dagster - DEBUG - download_latest_data_pipeline - b5c09304-46d3-491d-b84f-7f3c41ceb9c2 - 8628 - save_metadata.compute - OBJECT_STORE_OPERATION - Retrieved intermediate object for input ds_combined_compressed in memory object store using pickle.\n",
      "2021-01-07 19:02:10 - dagster - DEBUG - download_latest_data_pipeline - b5c09304-46d3-491d-b84f-7f3c41ceb9c2 - 8628 - save_metadata.compute - OBJECT_STORE_OPERATION - Retrieved intermediate object for input df_new_metadata in memory object store using pickle.\n",
      "2021-01-07 19:02:10 - dagster - DEBUG - download_latest_data_pipeline - b5c09304-46d3-491d-b84f-7f3c41ceb9c2 - 8628 - save_metadata.compute - STEP_INPUT - Got input \"ds_combined_compressed\" of type \"Any\". (Type check passed).\n",
      "2021-01-07 19:02:10 - dagster - DEBUG - download_latest_data_pipeline - b5c09304-46d3-491d-b84f-7f3c41ceb9c2 - 8628 - save_metadata.compute - STEP_INPUT - Got input \"df_new_metadata\" of type \"Any\". (Type check passed).\n",
      "2021-01-07 19:02:10 - dagster - DEBUG - download_latest_data_pipeline - b5c09304-46d3-491d-b84f-7f3c41ceb9c2 - 8628 - save_metadata.compute - STEP_INPUT - Got input \"table_id\" of type \"String\". (Type check passed).\n",
      "2021-01-07 19:02:10 - dagster - DEBUG - download_latest_data_pipeline - b5c09304-46d3-491d-b84f-7f3c41ceb9c2 - 8628 - save_metadata.compute - STEP_INPUT - Got input \"project_id\" of type \"String\". (Type check passed).\n",
      "2021-01-07 19:02:10 - dagster - DEBUG - download_latest_data_pipeline - b5c09304-46d3-491d-b84f-7f3c41ceb9c2 - 8628 - save_metadata.compute - STEP_OUTPUT - Yielded output \"result\" of type \"Any\". (Type check passed).\n",
      "2021-01-07 19:02:10 - dagster - DEBUG - download_latest_data_pipeline - b5c09304-46d3-491d-b84f-7f3c41ceb9c2 - 8628 - save_metadata.compute - OBJECT_STORE_OPERATION - Stored intermediate object for output result in memory object store using pickle.\n",
      "2021-01-07 19:02:10 - dagster - DEBUG - download_latest_data_pipeline - b5c09304-46d3-491d-b84f-7f3c41ceb9c2 - 8628 - save_metadata.compute - STEP_SUCCESS - Finished execution of step \"save_metadata.compute\" in 13ms.\n",
      "2021-01-07 19:02:10 - dagster - DEBUG - download_latest_data_pipeline - b5c09304-46d3-491d-b84f-7f3c41ceb9c2 - 8628 - ENGINE_EVENT - Finished steps in process (pid: 8628) in 10.37s\n",
      "2021-01-07 19:02:10 - dagster - DEBUG - download_latest_data_pipeline - b5c09304-46d3-491d-b84f-7f3c41ceb9c2 - 8628 - PIPELINE_SUCCESS - Finished execution of pipeline \"download_latest_data_pipeline\".\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<dagster.core.execution.results.PipelineExecutionResult at 0x16e8712dee0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_config = {\n",
    "    'solids': {\n",
    "        'download_eumetsat_files': {\n",
    "            'inputs': {\n",
    "                'data_dir': \"../data/raw\",\n",
    "                'metadata_db_fp': \"../data/EUMETSAT_metadata.db\",\n",
    "                'debug_fp': \"../logs/EUMETSAT_download.txt\",\n",
    "                'table_id': \"eumetsat.metadata\",\n",
    "                'project_id': \"solar-pv-nowcasting\",\n",
    "                'start_date': \"\",\n",
    "                'end_date': \"\"\n",
    "            },\n",
    "        },\n",
    "        'df_metadata_to_dt_to_fp_map': {\n",
    "            'inputs': {\n",
    "                'data_dir': \"../data/raw\"\n",
    "            }\n",
    "        },\n",
    "        'reproject_datasets': {\n",
    "            'inputs': {\n",
    "                'new_coords_fp': \"../data/intermediate/reproj_coords_TM_4km.csv\",\n",
    "                'new_grid_fp': \"../data/intermediate/new_grid_4km_TM.json\"\n",
    "            }\n",
    "        },\n",
    "        'compress_and_save_datasets': {\n",
    "            'inputs': {\n",
    "                'zarr_bucket': \"solar-pv-nowcasting-data/satellite/EUMETSAT/SEVIRI_RSS/full_extent_TM_int16\",\n",
    "                'var_name': \"stacked_eumetsat_data\"\n",
    "            }\n",
    "        },\n",
    "        'save_metadata': {\n",
    "            'inputs': {\n",
    "                'table_id': \"eumetsat.metadata\",\n",
    "                'project_id': \"solar-pv-nowcasting\"\n",
    "            },\n",
    "        },\n",
    "        'compress_export_then_delete_raw': {\n",
    "            'inputs': {\n",
    "                'data_dir': \"../data/raw\",\n",
    "                'compressed_dir': \"../data/compressed\",\n",
    "                'BUCKET_NAME': \"solar-pv-nowcasting-data\",\n",
    "                'PREFIX': \"satellite/EUMETSAT/SEVIRI_RSS/native/\"\n",
    "            },\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "execute_pipeline(download_latest_data_pipeline, run_config=run_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exports\n",
    "@solid(\n",
    "    config_schema = {\n",
    "        'user_key': Field(str, default_value=user_key, is_required=False),\n",
    "        'user_secret': Field(str, default_value=user_secret, is_required=False),\n",
    "        'slack_webhook_url': Field(str, default_value=slack_webhook_url, is_required=False),\n",
    "        'slack_id': Field(str, default_value=slack_id, is_required=False)\n",
    "    }\n",
    ")\n",
    "def download_missing_eumetsat_files(context, data_dir: str, metadata_db_fp: str, debug_fp: str, table_id: str, project_id: str, start_date: str='', end_date: str=''):\n",
    "    dm = eumetsat.DownloadManager(context.solid_config['user_key'], context.solid_config['user_secret'], data_dir, metadata_db_fp, debug_fp, slack_webhook_url=context.solid_config['slack_webhook_url'], slack_id=context.solid_config['slack_id'])\n",
    "    \n",
    "    missing_datasets = io.identifying_missing_datasets(start_date, end_date)[:5]\n",
    "    df_new_metadata = dm.download_datasets(missing_datasets)\n",
    "\n",
    "    if df_new_metadata is None:\n",
    "        df_new_metadata = pd.DataFrame(columns=['result_time', 'file_name'])\n",
    "    else:\n",
    "        df_new_metadata = df_new_metadata.iloc[1:] # the first entry is the last one we downloaded\n",
    "    \n",
    "    return df_new_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exports\n",
    "@pipeline\n",
    "def download_missing_data_pipeline():  \n",
    "    # Retrieving data, reprojecting, compressing, and saving to GCP\n",
    "    df_new_metadata = download_missing_eumetsat_files()\n",
    "    datetime_to_filepath = df_metadata_to_dt_to_fp_map(df_new_metadata)\n",
    "    ds_combined_reproj = reproject_datasets(datetime_to_filepath)\n",
    "    ds_combined_compressed = compress_and_save_datasets(ds_combined_reproj)\n",
    "    \n",
    "    save_metadata(ds_combined_compressed, df_new_metadata)\n",
    "    compress_export_then_delete_raw(ds_combined_compressed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'download_eumetsat_files' in run_config['solids'].keys():\n",
    "    run_config['solids']['download_missing_eumetsat_files'] = run_config['solids']['download_eumetsat_files']\n",
    "    run_config['solids'].pop('download_eumetsat_files')\n",
    "\n",
    "# execute_pipeline(download_missing_data_pipeline, run_config=run_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_utils.ipynb.\n",
      "Converted 01_eumetsat.ipynb.\n",
      "Converted 02_reprojection.ipynb.\n",
      "Converted 03_zarr.ipynb.\n",
      "Converted 04_gcp.ipynb.\n",
      "Converted 05_pipeline.ipynb.\n",
      "Converted 101_downloading.ipynb.\n",
      "Converted 102_reprojecting.ipynb.\n",
      "Converted 103_loading.ipynb.\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "from nbdev.export import *\n",
    "notebook2script()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
