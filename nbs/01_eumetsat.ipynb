{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp eumetsat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EUMETSAT API Wrapper Development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import dataset\n",
    "\n",
    "import FEAutils as hlp\n",
    "from typing import Union, List\n",
    "import xmltodict\n",
    "import dotenv\n",
    "import datetime\n",
    "import zipfile\n",
    "import copy\n",
    "import os\n",
    "from io import BytesIO\n",
    "import re\n",
    "import glob\n",
    "import logging\n",
    "import math\n",
    "import shutil\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "from requests.auth import HTTPBasicAuth\n",
    "import requests\n",
    "\n",
    "from nbdev.showdoc import *\n",
    "from fastcore.test import test\n",
    "\n",
    "from satip import utils\n",
    "from satip.gcp_helpers import get_eumetsat_filenames, upload_blob\n",
    "\n",
    "from ipypb import track\n",
    "from IPython.display import JSON"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### User Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '../data/raw'\n",
    "compressed_dir = '../data/compressed'\n",
    "debug_fp = '../logs/EUMETSAT_download.txt'\n",
    "env_vars_fp = '../.env'\n",
    "metadata_db_fp = '../data/EUMETSAT_metadata.db'\n",
    "\n",
    "download_data = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Authorising API Access\n",
    "\n",
    "First we'll load the the environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dotenv.load_dotenv(env_vars_fp)\n",
    "\n",
    "user_key = os.environ.get('USER_KEY')\n",
    "user_secret = os.environ.get('USER_SECRET')\n",
    "slack_id = os.environ.get('SLACK_ID')\n",
    "slack_webhook_url = os.environ.get('SLACK_WEBHOOK_URL')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "And test they were loaded successfully"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_env_vars_have_loaded(env_vars):\n",
    "    for name, value in env_vars.items():\n",
    "        assert value is not None, f'{name}` should not be None'\n",
    "    \n",
    "    return\n",
    "\n",
    "env_vars = {\n",
    "    'user_key': user_key,\n",
    "    'user_secret': user_secret,\n",
    "    'slack_id': slack_id,\n",
    "    'slack_webhook_url': slack_webhook_url\n",
    "}\n",
    "\n",
    "check_env_vars_have_loaded(env_vars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "We'll then use them to request an access token for the API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def request_access_token(user_key, user_secret):\n",
    "    \"\"\"\n",
    "    Requests an access token from the EUMETSAT data API\n",
    "    \n",
    "    Parameters:\n",
    "        user_key: EUMETSAT API key\n",
    "        user_secret: EUMETSAT API secret\n",
    "        \n",
    "    Returns:\n",
    "        access_token: API access token\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    token_url = 'https://api.eumetsat.int/token'\n",
    "\n",
    "    data = {\n",
    "      'grant_type': 'client_credentials'\n",
    "    }\n",
    "\n",
    "    r = requests.post(token_url, data=data, auth=(user_key, user_secret))\n",
    "    access_token = r.json()['access_token']\n",
    "\n",
    "    return access_token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "We'll then use them to request an access token for the API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "access_token = request_access_token(user_key, user_secret)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Querying Available Data\n",
    "\n",
    "Before we can download any data we have to know where it's stored. To learn this we can query their search-products API, which returns a JSON containing a list of file metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "format_dt_str = lambda dt: pd.to_datetime(dt).strftime('%Y-%m-%dT%H:%M:%SZ')\n",
    "\n",
    "def query_data_products(\n",
    "    start_date:str='2020-01-01', \n",
    "    end_date:str='2020-01-02', \n",
    "    start_index:int=0, \n",
    "    num_features:int=10_000,\n",
    "    product_id:str='EO:EUM:DAT:MSG:MSG15-RSS'\n",
    ") -> requests.models.Response:\n",
    "    \"\"\"\n",
    "    Queries the EUMETSAT data API for the specified data\n",
    "    product and date-range. The dates will accept any\n",
    "    format that can be interpreted by `pd.to_datetime`.\n",
    "    A maximum of 10,000 entries are returned by the API\n",
    "    so the indexes of the returned entries can be specified.\n",
    "    \n",
    "    Parameters:\n",
    "        start_date: Start of the query period\n",
    "        end_date: End of the query period\n",
    "        start_index: Starting index of returned entries\n",
    "        num_features: Number of returned entries\n",
    "        product_id: ID of the EUMETSAT product requested\n",
    "        \n",
    "    Returns:\n",
    "        r: Response from the request\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    search_url = 'https://api.eumetsat.int/data/search-products/os'\n",
    "\n",
    "    params = {\n",
    "        'format': 'json',\n",
    "        'pi': product_id,\n",
    "        'si': start_index, \n",
    "        'c': num_features,\n",
    "        'sort': 'start,time,0',\n",
    "        'dtstart': format_dt_str(start_date),\n",
    "        'dtend': format_dt_str(end_date)\n",
    "    }\n",
    "\n",
    "    r = requests.get(search_url, params=params)\n",
    "    \n",
    "    assert r.ok, f'Request was unsuccesful - Error code: {r.status_code}'\n",
    "    \n",
    "    return r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "We'll quickly make a test request to this end-point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = '2019-10-01'\n",
    "end_date = '2019-10-07'\n",
    "\n",
    "r = query_data_products(start_date, end_date)\n",
    "\n",
    "r_json = r.json()\n",
    "JSON(r_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "However the search-api is capped (at 10,000) for the number of files it will return metadata for, so we'll create a while loop that waits until all the relevant data has been returned. We'll then extract just the list of features from the returned JSONs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def identify_available_datasets(start_date: str, end_date: str, \n",
    "                                product_id='EO:EUM:DAT:MSG:MSG15-RSS'):\n",
    "    \"\"\"\n",
    "    Identifies available datasets from the EUMETSAT data\n",
    "    API for the specified data product and date-range. \n",
    "    The dates will accept any format that can be \n",
    "    interpreted by `pd.to_datetime`.\n",
    "    \n",
    "    Parameters:\n",
    "        start_date: Start of the query period\n",
    "        end_date: End of the query period\n",
    "        product_id: ID of the EUMETSAT product requested\n",
    "        \n",
    "    Returns:\n",
    "        r: Response from the request\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    num_features = 10000\n",
    "    start_index = 0\n",
    "    \n",
    "    datasets = []\n",
    "    all_results_returned = False\n",
    "    \n",
    "    while all_results_returned == False:\n",
    "        r_json = query_data_products(start_date, end_date, \n",
    "                                     start_index=start_index, \n",
    "                                     num_features=num_features, \n",
    "                                     product_id='EO:EUM:DAT:MSG:MSG15-RSS').json()\n",
    "\n",
    "        datasets += r_json['features']\n",
    "\n",
    "        num_total_results = r_json['properties']['totalResults']\n",
    "        num_returned_results = start_index + len(r_json['features'])\n",
    "\n",
    "        if num_returned_results < num_total_results:\n",
    "            start_index += num_features\n",
    "        else:\n",
    "            all_results_returned = True\n",
    "            \n",
    "        assert num_returned_results == len(datasets), 'Some features have not been appended'\n",
    "        \n",
    "    return datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "We'll check that the same number of available datasets are identified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "datasets = identify_available_datasets(start_date, end_date)\n",
    "\n",
    "print(f'{len(datasets)} datasets have been identified')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "Finally we'll create a helper function for converting the dataset ids into their file urls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "dataset_id_to_link = lambda data_id: f'https://api.eumetsat.int/data/download/products/{data_id}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "We'll now test this works.\n",
    "\n",
    "N.b. You cannot use the link returned here directly as it will not be OAuth'ed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_ids = sorted([dataset['id'] for dataset in datasets])\n",
    "example_data_link = dataset_id_to_link(dataset_ids[0])\n",
    "\n",
    "example_data_link"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Downloading Data\n",
    "\n",
    "Now that we know where our data is located we want to download it. First we'll check that the directory we wish to save the data in exists, if not we'll create it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for folder in [data_dir, sorted_dir]:\n",
    "    if not os.path.exists(folder):\n",
    "        os.makedirs(folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "We also want to extract the relevant metadata information from each file. Here we'll create a generalised framework for extracting data from any product, to add a new one please add its metadata mapping under the relevant `product_id`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def json_extract(json_obj:Union[dict, list], locators:list):\n",
    "    extracted_obj = copy.deepcopy(json_obj)\n",
    "    \n",
    "    for locator in locators:\n",
    "        extracted_obj = extracted_obj[locator]\n",
    "        \n",
    "    return extracted_obj\n",
    "\n",
    "def extract_metadata(data_dir: str, product_id='EO:EUM:DAT:MSG:MSG15-RSS'):\n",
    "    with open(f'{data_dir}/EOPMetadata.xml', 'r') as f:\n",
    "        xml_str = f.read()\n",
    "        \n",
    "    raw_metadata = xmltodict.parse(xml_str)\n",
    "    metadata_map = metadata_maps[product_id]\n",
    "    \n",
    "    datatypes_to_transform_func = {\n",
    "        'datetime': pd.to_datetime,\n",
    "        'str': str,\n",
    "        'int': int,\n",
    "        'float': float\n",
    "    }\n",
    "    \n",
    "    cleaned_metadata = dict()\n",
    "\n",
    "    for feature, attrs in metadata_map.items():\n",
    "        location = attrs['location']\n",
    "        datatype = attrs['datatype']\n",
    "\n",
    "        value = json_extract(raw_metadata, location)\n",
    "        formatted_value = datatypes_to_transform_func[datatype](value)\n",
    "\n",
    "        cleaned_metadata[feature] = formatted_value\n",
    "\n",
    "    return cleaned_metadata\n",
    "\n",
    "metadata_maps = {\n",
    "    'EO:EUM:DAT:MSG:MSG15-RSS': {\n",
    "        'start_date': {\n",
    "            'datatype': 'datetime',\n",
    "            'location': ['eum:EarthObservation', 'om:phenomenonTime', 'gml:TimePeriod', 'gml:beginPosition']\n",
    "        },\n",
    "        'end_date': {\n",
    "            'datatype': 'datetime',\n",
    "            'location': ['eum:EarthObservation', 'om:phenomenonTime', 'gml:TimePeriod', 'gml:endPosition']\n",
    "        },\n",
    "        'result_time': {\n",
    "            'datatype': 'datetime',\n",
    "            'location': ['eum:EarthObservation', 'om:resultTime', 'gml:TimeInstant', 'gml:timePosition']\n",
    "        },\n",
    "        'platform_short_name': {\n",
    "            'datatype': 'str',\n",
    "            'location': ['eum:EarthObservation', 'om:procedure', 'eop:EarthObservationEquipment', 'eop:platform', 'eop:Platform', 'eop:shortName']\n",
    "        },\n",
    "        'platform_orbit_type': {\n",
    "            'datatype': 'str',\n",
    "            'location': ['eum:EarthObservation', 'om:procedure', 'eop:EarthObservationEquipment', 'eop:platform', 'eop:Platform', 'eop:orbitType']\n",
    "        },\n",
    "        'instrument_name': {\n",
    "            'datatype': 'str',\n",
    "            'location': ['eum:EarthObservation', 'om:procedure', 'eop:EarthObservationEquipment', 'eop:instrument', 'eop:Instrument', 'eop:shortName']\n",
    "        },\n",
    "        'sensor_op_mode': {\n",
    "            'datatype': 'str',\n",
    "            'location': ['eum:EarthObservation', 'om:procedure', 'eop:EarthObservationEquipment', 'eop:sensor', 'eop:Sensor', 'eop:operationalMode']\n",
    "        },\n",
    "        'center_srs_name': {\n",
    "            'datatype': 'str',\n",
    "            'location': ['eum:EarthObservation', 'om:featureOfInterest', 'eop:Footprint', 'eop:centerOf', 'gml:Point', '@srsName']\n",
    "        },\n",
    "        'center_position': {\n",
    "            'datatype': 'str',\n",
    "            'location': ['eum:EarthObservation', 'om:featureOfInterest', 'eop:Footprint', 'eop:centerOf', 'gml:Point', 'gml:pos']\n",
    "        },\n",
    "        'file_name': {\n",
    "            'datatype': 'str',\n",
    "            'location': ['eum:EarthObservation', 'om:result', 'eop:EarthObservationResult', 'eop:product', 'eop:ProductInformation', 'eop:fileName', 'ows:ServiceReference', '@xlink:href']\n",
    "        },\n",
    "        'file_size': {\n",
    "            'datatype': 'int',\n",
    "            'location': ['eum:EarthObservation', 'om:result', 'eop:EarthObservationResult', 'eop:product', 'eop:ProductInformation', 'eop:size', '#text']\n",
    "        },\n",
    "        'missing_pct': {\n",
    "            'datatype': 'float',\n",
    "            'location': ['eum:EarthObservation', 'eop:metaDataProperty', 'eum:EarthObservationMetaData', 'eum:missingData', '#text']\n",
    "        },\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "We're now ready to create a download manager that will handle all of the querying, processing and retrieving for us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def check_valid_request(r: requests.models.Response):\n",
    "    \"\"\"\n",
    "    Checks that the response from the request is valid\n",
    "    \n",
    "    Parameters:\n",
    "        r: Response object from the request\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    class InvalidCredentials(Exception):\n",
    "        pass\n",
    "\n",
    "    if r.ok == False:\n",
    "        if 'Invalid Credentials' in r.text:\n",
    "            raise InvalidCredentials('The access token passed in the API request is invalid')\n",
    "        else:\n",
    "            raise Exception(f'The API request was unsuccesful {r.text} {r.status_code}')\n",
    "            \n",
    "    return\n",
    "\n",
    "class DownloadManager:\n",
    "    \"\"\"\n",
    "    The DownloadManager class provides a handler for downloading data\n",
    "    from the EUMETSAT API, managing: retrieval, logging and metadata\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, user_key: str, user_secret: str, \n",
    "                 data_dir: str, metadata_db_fp: str, log_fp: str, \n",
    "                 main_logging_level: str='DEBUG', slack_logging_level: str='CRITICAL', \n",
    "                 slack_webhook_url: str=None, slack_id: str=None, \n",
    "                 bucket_name=None, bucket_prefix=None, logger_name='EUMETSAT Download'):\n",
    "        \"\"\"\n",
    "        Initialises the download manager by:\n",
    "        * Setting up the logger\n",
    "        * Requesting an API access token\n",
    "        * Configuring the download directory\n",
    "        * Connecting to the metadata database\n",
    "        * Adding satip helper functions\n",
    "\n",
    "        Parameters:\n",
    "            user_key: EUMETSAT API key\n",
    "            user_secret: EUMETSAT API secret\n",
    "            data_dir: Path to the directory where the satellite data will be saved\n",
    "            metadata_db_fp: Path to where the metadata database is stored/will be saved\n",
    "            log_fp: Filepath where the logs will be stored\n",
    "            main_logging_level: Logging level for file and Jupyter\n",
    "            slack_logging_level: Logging level for Slack\n",
    "            slack_webhook_url: Webhook for the log Slack channel\n",
    "            slack_id: Option user-id to mention in Slack\n",
    "            bucket_name: (Optional) Google Cloud Storage bucket name to check for existing files\n",
    "            bucket_prefix: (Optional) Prefix for cloud bucket files\n",
    "\n",
    "        Returns:\n",
    "            download_manager: Instance of the DownloadManager class\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        # Configuring the logger\n",
    "        self.logger = utils.set_up_logging(logger_name, log_fp, \n",
    "                                           main_logging_level, slack_logging_level, \n",
    "                                           slack_webhook_url, slack_id)\n",
    "        \n",
    "        self.logger.info(f'********** Download Manager Initialised **************')\n",
    "        \n",
    "        # Requesting the API access token\n",
    "        self.user_key = user_key\n",
    "        self.user_secret = user_secret\n",
    "        \n",
    "        self.request_access_token()\n",
    "        \n",
    "        # Configuring the data directory\n",
    "        self.data_dir = data_dir\n",
    "        \n",
    "        if not os.path.exists(self.data_dir):\n",
    "            os.makedirs(self.data_dir)\n",
    "        \n",
    "        # Initialising the metadata database\n",
    "        self.metadata_db = dataset.connect(f'sqlite:///{metadata_db_fp}')\n",
    "        self.metadata_table = self.metadata_db['metadata']\n",
    "        \n",
    "        # Adding satip helper functions\n",
    "        self.identify_available_datasets = identify_available_datasets\n",
    "        self.query_data_products = query_data_products\n",
    "\n",
    "        # Google Cloud integration\n",
    "        self.bucket_name = bucket_name\n",
    "        self.bucket_prefix = bucket_prefix\n",
    "        self.bucket_filenames = None\n",
    "\n",
    "        if bucket_name:\n",
    "            print(f'Checking files in GCP bucket {bucket_name}, this will take a few seconds')\n",
    "            filenames = get_eumetsat_filenames(bucket_name, prefix=bucket_prefix)\n",
    "            self.bucket_filenames = [re.match(\"([A-Z\\d.]+-){6}\", filename)[0][:-1] for filename in filenames]\n",
    "        \n",
    "        return\n",
    "    \n",
    "    \n",
    "    def request_access_token(self, user_key=None, user_secret=None): \n",
    "        \"\"\"\n",
    "        Requests an access token from the EUMETSAT data API.\n",
    "        If no key or secret are provided then they will default\n",
    "        to the values provided in the download manager initialisation\n",
    "\n",
    "        Parameters:\n",
    "            user_key: EUMETSAT API key\n",
    "            user_secret: EUMETSAT API secret\n",
    "\n",
    "        Returns:\n",
    "            access_token: API access token\n",
    "\n",
    "        \"\"\"\n",
    "    \n",
    "        if user_key is None:\n",
    "            user_key = self.user_key\n",
    "        if user_secret is None:\n",
    "            user_secret = self.user_secret\n",
    "            \n",
    "        self.access_token = request_access_token(user_key, user_secret)\n",
    "        \n",
    "        return\n",
    "        \n",
    "    def download_single_dataset(self, data_link:str):\n",
    "        \"\"\"\n",
    "        Downloads a single dataset from the EUMETSAT API\n",
    "\n",
    "        Parameters:\n",
    "            data_link: Url link for the relevant dataset\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        params = {\n",
    "            'access_token': self.access_token\n",
    "        }\n",
    "\n",
    "        r = requests.get(data_link, params=params)\n",
    "        check_valid_request(r)\n",
    "\n",
    "        zipped_files = zipfile.ZipFile(BytesIO(r.content))\n",
    "        zipped_files.extractall(f'{self.data_dir}')\n",
    "\n",
    "        return\n",
    "\n",
    "\n",
    "    def check_if_downloaded(self, filenames: List[str]):\n",
    "        \"\"\"Checks which files should be downloaded based on \n",
    "        local file contents and a cloud storage bucket, if specified.\n",
    "        \n",
    "        Parameters:\n",
    "            filenames: List of filename strings\n",
    "        \n",
    "        Returns:\n",
    "            List of filenames to download\n",
    "        \"\"\"\n",
    "        in_bucket = []\n",
    "        local = []\n",
    "        download = []\n",
    "                \n",
    "        for filename in filenames:\n",
    "            # get first part of filename for matching\n",
    "            match = re.match(\"([A-Z\\d.]+-){6}\", filename)[0][:-1]\n",
    "            \n",
    "            if self.bucket_name:\n",
    "                if match in self.bucket_filenames:\n",
    "                    in_bucket.append(filename)\n",
    "                    if f'{filename}.nat' in os.listdir(self.data_dir):\n",
    "                        local.append(filename)\n",
    "                    continue\n",
    "\n",
    "            if f'{filename}.nat' in os.listdir(self.data_dir):\n",
    "                local.append(filename)\n",
    "                continue\n",
    "                \n",
    "            download.append(filename)\n",
    "\n",
    "        if self.bucket_name:\n",
    "            self.logger.info(f'{len(filenames)} files queried, {len(in_bucket)} found in bucket, {len(local)} found in {self.data_dir}, {len(download)} to download.')\n",
    "        else:\n",
    "            self.logger.info(f'{len(filenames)} files queried, {len(local)} found in {self.data_dir}, {len(download)} to download.')\n",
    "\n",
    "        return download, local\n",
    "\n",
    "\n",
    "    def download_date_range(self, start_date:str, end_date:str, product_id='EO:EUM:DAT:MSG:MSG15-RSS'):\n",
    "        \"\"\"\n",
    "        Downloads a set of dataset from the EUMETSAT API\n",
    "        in the defined date range and specified product\n",
    "\n",
    "        Parameters:\n",
    "            start_date: Start of the requested data period\n",
    "            end_date: End of the requested data period\n",
    "            product_id: ID of the EUMETSAT product requested\n",
    "        \n",
    "        \"\"\"\n",
    "    \n",
    "        datasets = identify_available_datasets(start_date, end_date, product_id=product_id)\n",
    "        df_new_metadata = self.download_datasets(datasets, product_id=product_id)\n",
    "\n",
    "        return df_new_metadata        \n",
    "\n",
    "\n",
    "    def download_datasets(self, datasets, product_id='EO:EUM:DAT:MSG:MSG15-RSS', download_all=True):\n",
    "        \"\"\"\n",
    "        Downloads a set of dataset from the EUMETSAT API\n",
    "        in the defined date range and specified product\n",
    "\n",
    "        Parameters:\n",
    "            datasets: list of datasets returned by `identify_available_datasets`\n",
    "        \n",
    "        \"\"\"\n",
    "\n",
    "        # Identifying dataset ids to download\n",
    "        dataset_ids = sorted([dataset['id'] for dataset in datasets])\n",
    "\n",
    "        # Check which datasets to download\n",
    "        download_ids, local_ids = self.check_if_downloaded(dataset_ids)\n",
    "        \n",
    "        # Downloading specified datasets\n",
    "        if not dataset_ids:\n",
    "            self.logger.info('No files will be downloaded. Set DownloadManager bucket_name argument for local download')\n",
    "            return \n",
    "        \n",
    "        all_metadata = []\n",
    "\n",
    "        for dataset_id in track(dataset_ids):\n",
    "            dataset_link = dataset_id_to_link(dataset_id)\n",
    "\n",
    "            # Download the raw data\n",
    "            if (dataset_id in download_ids) or (download_all == True):\n",
    "                try:\n",
    "                    self.download_single_dataset(dataset_link)\n",
    "                except:\n",
    "                    self.logger.info('The EUMETSAT access token has been refreshed')\n",
    "                    self.request_access_token()\n",
    "                    self.download_single_dataset(dataset_link)\n",
    "\n",
    "            # Extract and save metadata\n",
    "            dataset_metadata = extract_metadata(self.data_dir, product_id=product_id)\n",
    "            dataset_metadata.update({'downloaded': pd.Timestamp.now()})\n",
    "            all_metadata += [dataset_metadata]\n",
    "            self.metadata_table.insert(dataset_metadata)\n",
    "\n",
    "            # Delete old metadata files\n",
    "            for xml_file in ['EOPMetadata.xml', 'manifest.xml']:\n",
    "                xml_filepath = f'{self.data_dir}/{xml_file}'\n",
    "\n",
    "                if os.path.isfile(xml_filepath):\n",
    "                    os.remove(xml_filepath)\n",
    "                    \n",
    "        df_new_metadata = pd.DataFrame(all_metadata)\n",
    "                    \n",
    "        return df_new_metadata\n",
    "    \n",
    "    get_df_metadata = lambda self: pd.DataFrame(self.metadata_table.all()).set_index('id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "We'll now see what it looks like when we initialise the download manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dm = DownloadManager(user_key, user_secret, data_dir, metadata_db_fp, debug_fp, \n",
    "                     slack_webhook_url=slack_webhook_url, slack_id=slack_id)\n",
    "\n",
    "start_date = '2020-10-01 12:00'\n",
    "end_date = '2020-10-01 12:05'\n",
    "\n",
    "if download_data == True:\n",
    "    dm.download_date_range(start_date, end_date)\n",
    "\n",
    "df_metadata = dm.get_df_metadata()\n",
    "\n",
    "df_metadata.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "The `get_size` function was adapted from <a href=\"https://stackoverflow.com/questions/1392413/calculating-a-directorys-size-using-python\">this stackoverflow answer</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_dir_size(directory='.'):\n",
    "    total_size = 0\n",
    "    \n",
    "    for dirpath, dirnames, filenames in os.walk(directory):\n",
    "        for f in filenames:\n",
    "            fp = os.path.join(dirpath, f)\n",
    "            \n",
    "            if not os.path.islink(fp):\n",
    "                total_size += os.path.getsize(fp)\n",
    "\n",
    "    return total_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir_size = get_dir_size(data_dir)\n",
    "\n",
    "print(f'The data directory is currently {round(data_dir_size/1_000_000_000, 2):,} Gb')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "# Need to Clean Notebook Below Here\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Integration with Google Cloud Storage\n",
    "\n",
    "If Google Cloud Platform (GCP) flags are passed (`bucket_name` and `bucket_prefix`), when downloading, then the `DownloadManager` should first check to see if the files already exist in the specified cloud storage bucket.  \n",
    "If the files already exist, then they will not be downloaded locally - if the storage bucket arguments are passed.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUCKET_NAME = \"solar-pv-nowcasting-data\"\n",
    "PREFIX = \"satellite/EUMETSAT/SEVIRI_RSS/native/2020\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dm = DownloadManager(user_key, user_secret, data_dir, metadata_db_fp, debug_fp, bucket_name=BUCKET_NAME, bucket_prefix=PREFIX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bucket filenames can be accessed\n",
    "len(dm.bucket_filenames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets test this by examining some dates at the start of 2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Timings: around 2 hours to download 1 day.\n",
    "\n",
    "# DownloadManager should find these 2020 files on the VM\n",
    "start_date = '2020-01-01 00:00'\n",
    "end_date = '2020-01-02 00:00'\n",
    "dm.download_date_range(start_date, end_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Name Convention changes in EUMETSAT files\n",
    "\n",
    "Probably due to the changes / creation of the EUMETSAT API around the end of 2019, newer files do not contain the previous 'order number' parts at the end of the filename. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some previously downloaded files follow a different file name convention:\n",
    "\n",
    "Files through new API:\n",
    "`MSG3-SEVI-MSG15-0100-NA-20191001120415.883000000Z-NA.nat`  \n",
    "SatProgram-Instrument-SatNumber-AlgoVersion-InstrumentMode(?)-ReceptionStartDateUTC   \n",
    "\n",
    "Files on GCP:  \n",
    "` MSG3-SEVI-MSG15-0100-NA-20191001120415.883000000Z-20191001120433-1399526-1.nat.bz2`  \n",
    "SatProgram-Instrument-SatNumber-AlgoVersion-InstrumentMode(?)-ReceptionStartDateUTC-SensingStartDateUTC-OrderNumber-PartNumber "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use regex to take the first part of the filename for comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = \"MSG3-SEVI-MSG15-0100-NA-20190101000417.314000000Z-20190101000435-1377854-1.nat\"\n",
    "re.match(\"([A-Z\\d.]+-){6}\", txt)[0][:-1] # [:-1] to trim the trailing -"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To ensure we are comparing the same filenames, this regex is added into DownloadManager and the GCP helpers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up logging locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debug_fp = '../logs/EUMETSAT_download.txt'\n",
    "log = utils.set_up_logging('EUMETSAT Processing', debug_fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export \n",
    "def get_filesize_megabytes(filename):\n",
    "    filesize_bytes = os.path.getsize(filename)\n",
    "    return filesize_bytes / 1E6\n",
    "\n",
    "\n",
    "def eumetsat_filename_to_datetime(inner_tar_name):\n",
    "    p = re.compile('^MSG[23]-SEVI-MSG15-0100-NA-(\\d*)\\.')\n",
    "    title_match = p.match(inner_tar_name)\n",
    "    date_str = title_match.group(1)\n",
    "    return datetime.datetime.strptime(date_str, \"%Y%m%d%H%M%S\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For local testing, this command downloads some files from the Google Cloud bucket:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get test files from GCP - these are compressed\n",
    "# !gsutil cp -r gs://solar-pv-nowcasting-data/satellite/EUMETSAT/SEVIRI_RSS/native/2019/10/01/00/04 ../data/raw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compress downloaded native image files\n",
    "\n",
    "Once files have been downloaded from the EUMETSAT API in some location, they need to be compressed and saved in a cloud storage bucket. \n",
    "\n",
    "First, see which files have already been downloaded locally to test this functionality.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_native_filenames = glob.glob(os.path.join(data_dir, '*.nat'))\n",
    "full_native_filenames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will compress locally downloaded files here using `pbzip2`  \n",
    "On ubuntu: `sudo apt-get install -y pbzip2`  \n",
    "On mac: `brew install pbzip2`  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def compress_downloaded_files(data_dir, compressed_dir, log=None):\n",
    "    \"\"\"\n",
    "    Compresses downloaded files, stores them locally, \n",
    "    and ensures they are approximately the correct filesize.\n",
    "    Uses pbzip2 for compression.\n",
    "\n",
    "        Parameters:\n",
    "            data_dir: (string), directory path containing raw downloaded files from EUMETSAT API\n",
    "            compressed_dir: (string), directory path for compressed .nat files \n",
    "            log: (bool), flag to enable logging\n",
    "\n",
    "        Returns:\n",
    "            -\n",
    "    \"\"\"\n",
    "    NATIVE_FILESIZE_MB = 102.210123\n",
    "    EXTENSION = '.bz2'\n",
    "    \n",
    "    full_native_filenames = glob.glob(os.path.join(data_dir, '*.nat'))\n",
    "    print(f'Found {len(full_native_filenames)} native files.')\n",
    "    if log:\n",
    "        log.info(f'Found {len(full_native_filenames)} native files.')\n",
    "\n",
    "    for full_native_filename in full_native_filenames:\n",
    "        # Check filesize is correct\n",
    "        native_filesize_mb = get_filesize_megabytes(full_native_filename)\n",
    "\n",
    "        if not math.isclose(native_filesize_mb, NATIVE_FILESIZE_MB, abs_tol=1):\n",
    "            msg = f'Filesize incorrect for {full_native_filename}!  Expected {NATIVE_FILESIZE_MB} MB.  Actual = {native_filesize_mb} MB.'\n",
    "            if log:\n",
    "                log.error(msg)\n",
    "\n",
    "        if log:\n",
    "            log.debug('Compressing %s', full_native_filename)\n",
    "\n",
    "        completed_process = subprocess.run(['pbzip2', '-5', full_native_filename])\n",
    "        try:\n",
    "            completed_process.check_returncode()\n",
    "        except:\n",
    "            if log:\n",
    "                log.exception('Compression failed!')\n",
    "            print('Compression failed!')\n",
    "            raise\n",
    "\n",
    "        full_compressed_filename = full_native_filename + EXTENSION\n",
    "        compressed_filesize_mb = get_filesize_megabytes(full_compressed_filename)\n",
    "        if log:\n",
    "            log.debug(f'Filesizes: Before compression = {native_filesize_mb} MB. After compression = {compressed_filesize_mb} MB.  Compressed file is {compressed_filesize_mb/native_filesize_mb} x the size of the uncompressed file.')\n",
    "\n",
    "        base_native_filename = os.path.basename(full_native_filename)\n",
    "        dt = eumetsat_filename_to_datetime(base_native_filename)\n",
    "        \n",
    "        # Creating compressed_dir if not already made\n",
    "        if not os.path.exists(compressed_dir):\n",
    "            os.makedirs(compressed_dir)\n",
    "    \n",
    "        new_dst_path = os.path.join(compressed_dir, dt.strftime(\"%Y/%m/%d/%H/%M\"))\n",
    "        if not os.path.exists(new_dst_path):\n",
    "            os.makedirs(new_dst_path)\n",
    "\n",
    "        new_dst_full_filename = os.path.join(new_dst_path, base_native_filename + EXTENSION)\n",
    "        if log:\n",
    "            log.debug('Moving %s to %s', full_compressed_filename, new_dst_full_filename)\n",
    "        \n",
    "        if os.path.exists(new_dst_full_filename):\n",
    "            if log:\n",
    "                log.debug(f'{new_dst_full_filename} already exists.  Deleting old file')\n",
    "            os.remove(new_dst_full_filename)\n",
    "        shutil.move(src=full_compressed_filename, dst=new_dst_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compress_downloaded_files(data_dir=data_dir, compressed_dir=compressed_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Syncing files to GCP Storage\n",
    "\n",
    "Compressed native files should be stored in a Google Cloud Storage Bucket. The folder structure follows the following convention:  \n",
    "\n",
    "`gs://solar-pv-nowcasting-data/satellite/EUMETSAT/SEVIRI_RSS/native/<year>/<month>/<day>/<hour>/<minute>/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sync downloaded files in compressed_dir to the bucket\n",
    "BUCKET_NAME = \"solar-pv-nowcasting-data\"\n",
    "PREFIX = \"satellite/EUMETSAT/SEVIRI_RSS/native/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def upload_compressed_files(compressed_dir, BUCKET_NAME, PREFIX, log=None):\n",
    "    \"\"\"Uploads compressed native files to a Google Cloud storage bucket\n",
    "\n",
    "    For example,\n",
    "    compressed_dir:  /home/srv/data/intermediate/\n",
    "    bucket name: solar-pv-nowcasting-data\n",
    "    prefix:      satellite/EUMETSAT/SEVIRI_RSS/native/\n",
    "\n",
    "    With some files like:\n",
    "    /home/srv/data/intermediate/2018/01/01/01/23/04/MSG3-SEVI-MSG15-0100-NA-20191001120415.883000000Z-NA.nat.bz2\n",
    "\n",
    "    Would upload the files to:\n",
    "    gs://solar-pv-nowcasting-data/satellite/EUMETSAT/SEVIRI_RSS/native/2018/01/01/01/23/04/MSG3-SEVI-MSG15-0100-NA-20191001120415.883000000Z-NA.nat.bz2\n",
    "    etc\n",
    "\n",
    "        Parameters:\n",
    "            compressed_dir: (str), directory where compressed files are stored locally\n",
    "            BUCKET_NAME: (str), name of Google Cloud storage bucket\n",
    "            PREFIX: (str), string prefix to use as part of the bucket storage path\n",
    "\n",
    "        Returns:\n",
    "            -\n",
    "    \"\"\"\n",
    "    paths = Path(compressed_dir).rglob(\"*.nat.bz2\")\n",
    "    full_compressed_files = [x for x in paths if x.is_file()]\n",
    "    if log:\n",
    "        log.info(f\"Found {len(full_compressed_files)} compressed files.\")\n",
    "\n",
    "    for file in full_compressed_files:\n",
    "        rel_path = os.path.relpath(file.absolute(), compressed_dir)\n",
    "        upload_blob(\n",
    "            bucket_name=BUCKET_NAME,\n",
    "            source_file_name=file.absolute(),\n",
    "            destination_blob_name=rel_path,\n",
    "            prefix=PREFIX,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.export import *\n",
    "notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "satip_dev",
   "language": "python",
   "name": "satip_dev"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
